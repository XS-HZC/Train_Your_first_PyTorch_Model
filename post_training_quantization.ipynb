{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# pathlib：是 Python 标准库中的一个模块，提供了面向对象的文件系统路径操作方法\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子，确保实验的可重复性\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # 归一化处理：使用均值 0.1307 和标准差 0.3081 对图像进行归一化\n",
    "    # MNIST 全体像素均值：0.1307\n",
    "    # MNIST 全体像素标准差：0.3081\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    # MNIST 是灰度图像，因此均值和标准差都是单通道的元组\n",
    "    # 如果是 RGB 彩色图像，则需要提供三个通道的均值和标准差，例如：((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    # transforms.Normalize((mean_R, mean_G, mean_B), (std_R, std_G, std_B))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load the MNIST test set\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerySimpleNet(nn.Module):\n",
    "    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n",
    "        super(VerySimpleNet,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_size_1) \n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n",
    "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VerySimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此训练方法仅针对分类任务\n",
    "# output 的 shape = [batch_size, num_classes]（每类的 logits）\n",
    "# y 的 shape = [batch_size]，每个元素是类别索引（整数）\n",
    "def train(train_loader, model, epochs=5, total_iterations_limit=None):\n",
    "    # total_iterations_limit：如果指定了该参数，则训练将在达到该总迭代次数后提前终止\n",
    "    model.to(device)\n",
    "\n",
    "    cross_el = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # 统计整个训练过程中已经执行了多少次迭代（跨 epoch 累加）\n",
    "    total_iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        # loss_sum 用于累计每个 batch 的 loss\n",
    "        loss_sum = 0\n",
    "        # num_iterations 用于统计当前 epoch 已经执行了多少次迭代\n",
    "        num_iterations = 0\n",
    "\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "\n",
    "        # 如果指定了 total_iterations_limit，则调整 data_iterator 的总迭代次数\n",
    "        if total_iterations_limit is not None and total_iterations_limit - total_iterations < data_iterator.total:\n",
    "            data_iterator.total = total_iterations_limit - total_iterations\n",
    "\n",
    "        # tqdm 的进度条 每执行一次 for data in data_iterator: 的循环，进度条就增加 1\n",
    "        for data in data_iterator:\n",
    "\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 使用其他模型时，可能需要调整这里的输入格式\n",
    "            output = model(x.view(-1, 28*28))\n",
    "\n",
    "            loss = cross_el(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "\n",
    "            # 在 tqdm 进度条后面显示额外信息\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
    "\n",
    "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1024)\n",
    "        # os.path.getsize(...) → 文件大小，单位 Bytes\n",
    "        # 1 Byte = 8 bits\n",
    "        # /1024 → 转换为 KB\n",
    "\n",
    "    os.remove('temp_delme.p')\n",
    "\n",
    "MODEL_FILENAME = 'simplenet_ptq.pt'\n",
    "\n",
    "if Path(MODEL_FILENAME).exists():\n",
    "    net.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "    print('Loaded model from disk')\n",
    "else:\n",
    "    train(train_loader, net, epochs=5)\n",
    "    # Save the model to disk\n",
    "    torch.save(net.state_dict(), MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此测试方法仅针对分类任务\n",
    "# output 的 shape = [batch_size, num_classes]（每类的 logits）\n",
    "# y 的 shape = [batch_size]，每个元素是类别索引（整数\n",
    "def test(test_loader, model: nn.Module, total_iterations: int = None):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # 使用其他模型时，可能需要调整这里的输入格式\n",
    "            output = model(x.view(-1, 28*28))\n",
    "            \n",
    "            # enumerate 接受一个可迭代对象（列表、张量、生成器等）,返回 索引和元素 的对 (index, element)\n",
    "            for idx, i in enumerate(output):\n",
    "                # torch.argmax(i) → 返回张量 i 中最大值的索引\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                total +=1\n",
    "            iterations += 1\n",
    "            if total_iterations is not None and iterations >= total_iterations:\n",
    "                break\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy: {round(accuracy * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print weights and size of the model before quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights matrix of the model before quantization\n",
    "print('Weights before quantization')\n",
    "print(net.linear1.weight)\n",
    "print(net.linear1.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of the model before quantization')\n",
    "print_size_of_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy of the model before quantization: ')\n",
    "test(test_loader, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert min-max observers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedVerySimpleNet(nn.Module):\n",
    "    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n",
    "        super(QuantizedVerySimpleNet,self).__init__()\n",
    "        # torch.quantization.QuantStub() 本身是一个 nn.Module，后续会被替换为真正的量化操作，在 PyTorch 的 静态量化（Static Quantization） 中用来 占位（Stub）\n",
    "        # 占位 Stub：一个临时占据位置的对象，在模型定义时放进去，但在实际推理前会被 PyTorch 自动 替换为具体操作（如量化、反量化等）\n",
    "        # 在 prepare() 之后会插入一个 Observer，用于收集输入的 min/max 值\n",
    "        # 在 convert() 之后会变成真正的量化操作（如 torch.quantize_per_tensor）\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_size_1) \n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n",
    "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        # 同上\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.quant(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# class VerySimpleNet(nn.Module):\n",
    "#     def __init__(self, hidden_size_1=100, hidden_size_2=100):\n",
    "#         super(VerySimpleNet,self).__init__()\n",
    "#         self.linear1 = nn.Linear(28*28, hidden_size_1) \n",
    "#         self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n",
    "#         self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         x = img.view(-1, 28*28)\n",
    "#         x = self.relu(self.linear1(x))\n",
    "#         x = self.relu(self.linear2(x))\n",
    "#         x = self.linear3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_quantized = QuantizedVerySimpleNet()\n",
    "# Copy weights from unquantized model\n",
    "net_quantized.load_state_dict(net.state_dict())\n",
    "net_quantized.eval()\n",
    "\n",
    "# 给模型设置 默认量化配置（qconfig）\n",
    "# 激活使用 MinMaxObserver\n",
    "# 权重使用 PerChannelMinMaxObserver（逐通道量化）\n",
    "# 数据类型为 int8\n",
    "net_quantized.qconfig = torch.ao.quantization.default_qconfig\n",
    "\n",
    "# 向模型中插入 “观察器（observer）” 模块，用于记录训练/推理过程中每个层的激活最小值和最大值\n",
    "# prepare() 会执行以下操作：\n",
    "# 用 observer 模块替换掉 QuantStub 和 DeQuantStub\n",
    "# 给每层的输入输出插入观察器（记录 min/max）\n",
    "# 模型仍为 float 形式，但能在 calibration 阶段收集数据分布\n",
    "# 当自定义模块或其子模块显式设置了 .qconfig，torch.ao.quantization.prepare() 才会在其中插入观察器（observer）\n",
    "net_quantized = torch.ao.quantization.prepare(net_quantized) # Insert observers\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrate the model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize the model using the statistics collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_quantized.to(\"cpu\")\n",
    "# 替换观察器模块为实际的量化和反量化操作之前必须在 CPU 上\n",
    "net_quantized = torch.ao.quantization.convert(net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print weights of the model after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights matrix of the model after quantization\n",
    "print('Weights after quantization')\n",
    "# torch.int_repr() 返回量化后的整数表示\n",
    "print(torch.int_repr(net_quantized.linear1.weight()))\n",
    "print(net_quantized.linear1.weight())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the dequantized weights and the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original weights: ')\n",
    "print(net.linear1.weight)\n",
    "print('')\n",
    "print(f'Dequantized weights: ')\n",
    "# torch.dequantize(...) → 把量化权重转回 float32\n",
    "print(torch.dequantize(net_quantized.linear1.weight()))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print size and accuracy of the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of the model after quantization')\n",
    "print_size_of_model(net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(net_quantized))\n",
    "print(net_quantized)\n",
    "print(net_quantized.linear1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantizedtest(test_loader, model: nn.Module, total_iterations: int = None):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(\"cpu\")\n",
    "            y = y.to(\"cpu\")\n",
    "\n",
    "            # 使用其他模型时，可能需要调整这里的输入格式\n",
    "            output = model(x.view(x.size(0), -1))\n",
    "            \n",
    "            # enumerate 接受一个可迭代对象（列表、张量、生成器等）,返回 索引和元素 的对 (index, element)\n",
    "            for idx, i in enumerate(output):\n",
    "                # torch.argmax(i) → 返回张量 i 中最大值的索引\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                total +=1\n",
    "            iterations += 1\n",
    "            if total_iterations is not None and iterations >= total_iterations:\n",
    "                break\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy: {round(accuracy * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing the model after quantization')\n",
    "Quantizedtest(test_loader, net_quantized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
